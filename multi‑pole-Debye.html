<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Multi-Pole Debye Model Fitter</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/pyodide/v0.27.0/full/pyodide.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.32.0.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .spinner {
            border-top-color: #3498db;
            animation: spin 1s linear infinite;
        }
        @keyframes spin {
            to { transform: rotate(360deg); }
        }
        .btn-disabled {
            background-color: #9ca3af;
            cursor: not-allowed;
            opacity: 0.7;
        }
        /* Style for parameter sliders */
        input[type=range] {
            -webkit-appearance: none;
            appearance: none;
            width: 100%;
            height: 8px;
            background: #d1d5db;
            border-radius: 5px;
            outline: none;
            opacity: 0.7;
            transition: opacity .2s;
        }
        input[type=range]:hover {
            opacity: 1;
        }
        input[type=range]::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 20px;
            height: 20px;
            background: #3b82f6;
            cursor: pointer;
            border-radius: 50%;
        }
        input[type=range]::-moz-range-thumb {
            width: 20px;
            height: 20px;
            background: #3b82f6;
            cursor: pointer;
            border-radius: 50%;
        }
        .pole-controls {
            border-left: 3px solid #3b82f6;
            padding-left: 1rem;
            margin-bottom: 1.5rem;
        }
    </style>
</head>
<body class="bg-gray-100 text-gray-800">

    <div class="container mx-auto p-4 md:p-8 max-w-7xl">
        <header class="text-center mb-8">
            <h1 class="text-3xl md:text-4xl font-bold text-gray-900">Multi-Pole Debye Model Fitter</h1>
            <p class="mt-2 text-lg text-gray-600">Upload your dielectric spectroscopy data (CSV) to fit it to the Multi-Pole Debye model.</p>
        </header>

        <div class="bg-white rounded-xl shadow-lg p-6 md:p-8">
            <!-- Controls Section -->
            <div class="grid grid-cols-1 md:grid-cols-2 gap-6 items-center mb-6">
                <div>
                    <label for="file-upload" class="block text-sm font-medium text-gray-700 mb-2">
                        Upload CSV File
                    </label>
                    <div class="flex items-center space-x-4">
                        <input type="file" id="file-upload" accept=".csv" class="block w-full text-sm text-gray-500
                            file:mr-4 file:py-2 file:px-4
                            file:rounded-lg file:border-0
                            file:text-sm file:font-semibold
                            file:bg-blue-50 file:text-blue-700
                            hover:file:bg-blue-100
                        ">
                    </div>
                    <p class="text-xs text-gray-500 mt-2">
                        CSV columns in order:
                        Frequency&nbsp;(GHz),&nbsp;Dk&nbsp;(real&nbsp;permittivity),
                        Df&nbsp;(loss&nbsp;tangent&nbsp;<em>tan δ</em>)
                    </p>

                </div>
                <div class="space-y-4">
                    <div>
                        <label for="n-poles" class="block text-sm font-medium text-gray-700">Number of Debye Poles</label>
                        <select id="n-poles" class="mt-1 block w-full pl-3 pr-10 py-2 text-base border-gray-300 focus:outline-none focus:ring-blue-500 focus:border-blue-500 sm:text-sm rounded-md">
                            <option value="auto" selected>Auto-detect</option>
                            <option value="1">1 Pole</option>
                            <option value="2">2 Poles</option>
                            <option value="3">3 Poles</option>
                            <option value="4">4 Poles</option>
                        </select>
                    </div>
                    <div>
                        <label for="fit-method" class="block text-sm font-medium text-gray-700">Optimization Method</label>
                        <select id="fit-method" class="mt-1 block w-full pl-3 pr-10 py-2 text-base border-gray-300 focus:outline-none focus:ring-blue-500 focus:border-blue-500 sm:text-sm rounded-md">
                            <option value="least_squares" selected>least_squares (Default)</option>
                            <option value="nelder">nelder</option>
                            <option value="lbfgsb">lbfgsb</option>
                        </select>
                    </div>
                    <button id="run-button" class="w-full bg-blue-600 text-white font-bold py-3 px-6 rounded-lg shadow-md hover:bg-blue-700 transition-colors duration-300 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-opacity-50">
                        Run Analysis
                    </button>
                </div>
            </div>

            <!-- Status and Output Section -->
            <div id="status" class="text-center my-4 p-4 bg-gray-50 rounded-lg hidden">
                <div class="flex items-center justify-center">
                    <div class="spinner w-6 h-6 rounded-full border-4 border-gray-300"></div>
                    <p id="status-message" class="ml-4 text-gray-700 font-medium"></p>
                </div>
            </div>

            <div id="output" class="hidden mt-8">
                <div class="grid grid-cols-1 lg:grid-cols-3 gap-8">
                    <!-- Plot Display -->
                    <div id="plot-container" class="lg:col-span-2 space-y-4">
                        <div class="border border-gray-200 rounded-lg bg-gray-50 p-2">
                            <div id="plot-output"></div>
                        </div>
                        <!-- BIC Chart -->
                        <div class="border border-gray-200 rounded-lg bg-gray-50 p-2 hidden" id="bic-chart-container">
                            <div id="bic-chart" style="height: 300px;"></div>
                        </div>
                    </div>

                    <!-- Parameter Controls -->
                    <div id="controls-panel" class="lg:col-span-1 bg-white p-6 rounded-lg border">
                        <div class="flex justify-between items-center mb-4">
                            <h3 class="text-xl font-bold text-gray-800">Adjust Fit Parameters</h3>
                            <div class="flex space-x-2">
                                <button id="reset-button" class="text-sm bg-gray-200 hover:bg-gray-300 text-gray-800 font-semibold py-1 px-3 rounded-lg">
                                    Reset
                                </button>
                                <button id="download-button" class="text-sm bg-blue-500 hover:bg-blue-600 text-white font-semibold py-1 px-3 rounded-lg">
                                    Download
                                </button>
                            </div>
                        </div>

                        <!-- Fit Quality Indicator -->
                        <div id="fit-quality-indicator" class="mb-4 p-3 rounded-lg hidden">
                            <div class="flex items-center justify-between">
                                <span class="text-sm font-medium">Fit Quality:</span>
                                <span id="fit-quality-score" class="text-lg font-bold"></span>
                            </div>
                            <div class="w-full bg-gray-200 rounded-full h-2 mt-2">
                                <div id="fit-quality-bar" class="h-2 rounded-full transition-all duration-300"></div>
                            </div>
                            <!-- Warnings Display -->
                            <div id="fit-warnings" class="mt-3 space-y-1 hidden">
                                <div class="text-xs font-medium text-gray-600 mb-1">Warnings:</div>
                                <div id="warnings-container" class="space-y-1"></div>
                            </div>
                        </div>

                        <div id="params-container" class="space-y-6">
                            <!-- eps_inf -->
                            <div>
                                <label for="eps_inf_slider" class="block text-sm font-medium text-gray-700">ε<sub>∞</sub> (Epsilon Infinity)</label>
                                <input type="range" id="eps_inf_slider" class="mt-1">
                                <input type="number" id="eps_inf_input" class="mt-2 w-full p-2 border border-gray-300 rounded-md text-sm">
                            </div>
                            <!-- Dynamic pole controls will be added here -->
                            <div id="pole-controls-container"></div>
                        </div>
                    </div>
                </div>

                <!-- Report Display -->
                <div class="mt-8">
                    <h2 class="text-2xl font-bold mb-4 text-gray-800">Fit Report</h2>
                    <div class="bg-gray-900 text-white font-mono text-sm p-6 rounded-lg overflow-x-auto">
                        <pre id="report-output"></pre>
                    </div>
                </div>
            </div>
             <!-- Error Display -->
            <div id="error-box" class="hidden mt-6 p-4 bg-red-100 border border-red-400 text-red-700 rounded-lg">
                <h3 class="font-bold">An Error Occurred</h3>
                <p id="error-message"></p>
            </div>
        </div>
        <footer class="text-center mt-8 text-sm text-gray-500">
            <p>Powered by Pyodide, lmfit, and plotly</p>
            <div class="border-t border-gray-300 pt-2 mt-2">
            <p class="font-semibold text-gray-700">Citation</p>
            <p class="italic">Meddeb, B., & Meddeb, A. (2025). QuickFitter: Interactive Dielectric Data Fitting Tool - Multi-Pole Debye Model.
            <br>Available at: <a href="https://github.com/bmeddeb/QuickFitter" class="text-blue-600 hover:underline">https://github.com/bmeddeb/QuickFitter</a></p>
        </div>
        </footer>
    </div>

    <script type="text/javascript">
        // DOM Elements
        const fileUpload = document.getElementById('file-upload');
        const runButton = document.getElementById('run-button');
        const resetButton = document.getElementById('reset-button');
        const downloadButton = document.getElementById('download-button');
        const fitMethodSelect = document.getElementById('fit-method');
        const nPolesSelect = document.getElementById('n-poles');
        const statusDiv = document.getElementById('status');
        const statusMessage = document.getElementById('status-message');
        const outputDiv = document.getElementById('output');
        const plotOutput = document.getElementById('plot-output');
        const reportOutput = document.getElementById('report-output');
        const errorBox = document.getElementById('error-box');
        const errorMessage = document.getElementById('error-message');
        const poleControlsContainer = document.getElementById('pole-controls-container');

        // Global State
        let pyodide = null;
        let fileContent = null;
        let isPyodideReady = false;
        let currentData = null;
        let originalFilename = '';
        let currentPoleControls = {};
        let currentNPoles = 0;

        // Python Script
        const pythonScript = `
import sys
import io
import json
from datetime import datetime
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.io as pio
from lmfit import Minimizer, Parameters, fit_report
import math
import logging
from typing import Dict, Tuple, Any, List, Optional
from dataclasses import dataclass, field
from scipy.signal import find_peaks
import numdifftools as nd

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Fit Evaluator Classes (same as before with adapted thresholds)
@dataclass
class MetricResult:
    """Stores the evaluation result for a single metric."""
    name: str
    score: float
    category: str
    value: float
    suggestion: str | None = None
    details: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ReportEvaluation:
    """Stores the full evaluation for a report."""
    metrics: Dict[str, MetricResult]
    overall: MetricResult
    suggestions: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)

    def to_markdown(self) -> str:
        """Return the evaluation as a markdown-formatted table."""
        headers = ["Metric", "Score", "Category", "Value", "Suggestion"]
        lines = ["| " + " | ".join(headers) + " |",
                 "| " + " | ".join(["---"]*len(headers)) + " |"]
        for m in self.metrics.values():
            suggestion = m.suggestion or ""
            lines.append(f"| {m.name} | {m.score} | {m.category} | {m.value:.4g} | {suggestion} |")
        lines.append(
            f"| **Overall** | {self.overall.score} | {self.overall.category} | {self.overall.value} |  |"
        )
        if self.warnings:
            lines.append("\\n**Warnings:**")
            for warning in self.warnings:
                lines.append(f"- {warning}")
        return "\\n".join(lines)

    def to_dict(self) -> Dict[str, Any]:
        """Convert evaluation to dictionary format."""
        return {
            'overall_score': self.overall.score,
            'overall_category': self.overall.category,
            'metrics': {
                k: {
                    'score': v.score,
                    'category': v.category,
                    'value': v.value,
                    'suggestion': v.suggestion,
                    'details': v.details
                }
                for k, v in self.metrics.items()
            },
            'suggestions': self.suggestions,
            'warnings': self.warnings,
            'markdown_table': self.to_markdown()
        }

class FitEvaluator:
    """Evaluates the quality of a fit report based on configurable metrics."""
    DEFAULT_THRESHOLDS: Dict[str, Tuple[float, float, float, str]] = {
        "reduced_chi_square": (1e-4, 1e-3, 1e-2, "Consider adding more poles or adjusting relaxation times."),
        "mean_residual":      (1e-4, 5e-4, 1e-3, "Check for systematic bias, consider frequency-dependent effects."),
        "aic":                (2, 7, 14, "Consider reducing number of poles."),
        "bic":                (2, 7, 14, "Model complexity not justified by data."),
    }

    DEFAULT_WEIGHTS: Dict[str, int] = {
        'dpv': 1,
        'reduced_chi_square': 3,
        'rms_real': 2,
        'rms_imag': 2,
        'mean_residual': 1,
        'correlation': 2,
        'aic': 2,
        'bic': 2
    }

    def __init__(self, thresholds=None, weights=None):
        self.thresholds = thresholds or FitEvaluator.DEFAULT_THRESHOLDS
        self.weights = weights or FitEvaluator.DEFAULT_WEIGHTS

    def _dynamic_rms_thresholds(self, report):
        """Generate data-dependent RMS thresholds based on median and peak values."""
        # Use median for real part
        med_dk = report.get('median_dk', 1)  # fallback
        good_rms_real = 0.10 * med_dk

        # Use peak for imaginary part (more robust for high-loss samples)
        peak_df = report.get('peak_df', 0.05)  # fallback
        good_rms_imag = 0.03 * peak_df
        excellent_rms_imag = 0.015 * peak_df
        godly_rms_imag = 0.0075 * peak_df

        return {
            'rms_real': (0.02*med_dk, 0.05*med_dk, good_rms_real,
                         "Refine ε∞ or Δε_i."),
            'rms_imag': (godly_rms_imag, excellent_rms_imag, good_rms_imag,
                         "Add pole or adjust τ_i.")
        }

    @classmethod
    def from_json_report(cls, json_data: Dict[str, Any]) -> Dict[str, Any]:
        """Convert JSON report format to evaluator format."""
        fit_stats = json_data.get('fit_statistics', {})
        residuals = json_data.get('residual_analysis', {})
        corr_matrix = json_data.get('correlation_matrix', [])
        correlations = {}
        if corr_matrix and isinstance(corr_matrix, list):
            n_poles = json_data.get('parameters', {}).get('n_poles', 1)
            param_names = ['eps_inf']
            for i in range(n_poles):
                param_names.extend([f'delta_eps_{i}', f'tau_{i}'])
            try:
                for i in range(len(corr_matrix)):
                    for j in range(i+1, len(corr_matrix[0])):
                        if abs(corr_matrix[i][j]) > 0.1:
                            correlations[(param_names[i], param_names[j])] = corr_matrix[i][j]
            except (IndexError, TypeError) as e:
                logger.warning(f"Error parsing correlation matrix: {e}")

        # Include parameters for physical validation
        params = json_data.get('parameters', {})

        return {
            'n_data_points': fit_stats.get('n_data_points', 0),
            'n_variables': fit_stats.get('n_variables', 4),
            'reduced_chi_square': fit_stats.get('reduced_chi_square', 0),
            'rms_real': residuals.get('real_part', {}).get('rms', 0),
            'rms_imag': residuals.get('imaginary_part', {}).get('rms', 0),
            'mean_real': residuals.get('real_part', {}).get('mean', 0),
            'mean_imag': residuals.get('imaginary_part', {}).get('mean', 0),
            'aic': json_data.get('information_criteria', {}).get('aic', 0),
            'bic': json_data.get('information_criteria', {}).get('bic', 0),
            'correlations': correlations,
            'parameters': params,
            'eps_inf': params.get('eps_inf', 0),
            'n_poles': params.get('n_poles', 1),
            'median_dk': json_data.get('statistics', {}).get('median_dk', 1),
            'median_df': json_data.get('statistics', {}).get('median_df', 0.01),
            'peak_df': json_data.get('statistics', {}).get('peak_df', 0.05),
        }

    def _validate_report(self, report: Dict[str, Any]) -> None:
        """Validate report data types and values."""
        try:
            n_data = int(report['n_data_points'])
            n_vars = int(report['n_variables'])
            if n_vars <= 0:
                raise ValueError("n_variables must be positive")
            if n_data <= n_vars:
                raise ValueError("n_data_points must exceed n_variables")
            numeric_fields = ['reduced_chi_square', 'rms_real', 'rms_imag', 'mean_real', 'mean_imag']
            for field in numeric_fields:
                val = report.get(field, 0)
                if not isinstance(val, (int, float)) or math.isnan(val):
                    raise ValueError(f"{field} must be a valid number")
        except (TypeError, ValueError, KeyError) as e:
            raise ValueError(f"Invalid report data: {e}")

    def _validate_physical_bounds(self, report: Dict[str, Any]) -> List[str]:
        """Check if values are physically reasonable for Multi-Pole Debye."""
        warnings = []
        if report.get('rms_real', 0) > 1.0:
            warnings.append("RMS real part unusually high - check data quality")
        if report.get('rms_imag', 0) > 0.1:
            warnings.append("RMS imaginary part very high - check loss data quality")
        rcs = report.get('reduced_chi_square', 0)
        if rcs < 1e-6 and rcs > 0:
            warnings.append("Reduced chi-square suspiciously low - possible overfitting")
        elif rcs > 1.0:
            warnings.append("Reduced chi-square > 1 - consider adding more poles")

        # Check epsilon infinity
        eps_inf = report.get('eps_inf', 0)
        if eps_inf < 1.0:
            warnings.append("ε_∞ < 1.0 is unusual for dielectric materials")
        elif eps_inf > 1000:
            warnings.append("ε_∞ > 1000 is unusually high - check units and data")

        # Check number of poles vs data points
        n_poles = report.get('n_poles', 1)
        n_data_points = report.get('n_data_points', 0)
        if n_poles > 0 and n_data_points > 0:
            params_per_pole = 2  # delta_eps and tau for each pole
            total_params = 1 + n_poles * params_per_pole  # eps_inf + poles
            if total_params > n_data_points / 3:
                warnings.append(f"Too many poles ({n_poles}) for data size - risk of overfitting")

        # Check for relaxation time constants in parameters
        params = report.get('parameters', {})
        for i in range(n_poles):
            tau = params.get(f'tau_{i}', None)
            if tau is not None and tau <= 0:
                warnings.append(f"Non-positive τ_{i} detected - unphysical relaxation time")

        return warnings

    def _evaluate_scalar(self, value: float, thresholds: Tuple[float, float, float]) -> Tuple[int, str]:
        """Evaluates metrics where smaller values are better."""
        godly_max, excellent_max, good_max = thresholds
        if value < godly_max:
            return 100, "Godly"
        elif value < excellent_max:
            return 80, "Excellent"
        elif value < good_max:
            return 60, "Good"
        return 30, "Poor"

    def _evaluate_inverse_scalar(self, value: float, thresholds: Tuple[float, float, float]) -> Tuple[int, str]:
        """Evaluates metrics where larger values are better."""
        poor_min, good_min, excellent_min = thresholds
        if value >= excellent_min:
            return 100, "Godly"
        elif value >= good_min:
            return 80, "Excellent"
        elif value >= poor_min:
            return 60, "Good"
        return 30, "Poor"

    def _evaluate_correlation(self, corr_value: float) -> Tuple[int, str, str | None]:
        """Evaluates correlation, returning score, category, and suggestion."""
        if not isinstance(corr_value, (int, float)) or math.isnan(corr_value):
            return 30, "Invalid", "Check correlation calculation"
        abs_corr = abs(corr_value)
        if abs_corr < 0.50:
            return 100, "Low", None
        elif abs_corr < 0.80:
            return 80, "Moderate", None
        elif abs_corr < 0.95:
            return 60, "High", "Consider merging poles or fixing parameters."
        return 30, "Very High", "Poles may be redundant - reduce number of poles."

    def evaluate(self, report: Dict[str, Any]) -> ReportEvaluation:
        """Validates and evaluates a fit report dictionary."""
        required = {
            'n_data_points', 'n_variables', 'reduced_chi_square',
            'rms_real', 'rms_imag', 'mean_real', 'mean_imag', 'correlations'
        }
        missing = required - report.keys()
        if missing:
            raise KeyError(f"Missing required report fields: {missing}")

        self._validate_report(report)
        warnings = self._validate_physical_bounds(report)

        metrics: Dict[str, MetricResult] = {}
        suggestions: List[str] = []

        mean_res = max(abs(report['mean_real']), abs(report['mean_imag']))
        report_vals = dict(report)
        report_vals['mean_residual'] = mean_res

        # Inject data-dependent thresholds
        dyn = self._dynamic_rms_thresholds(report)
        local_thresholds = {**self.thresholds, **dyn}

        # Data-points-per-variable
        dpv = report['n_data_points'] / report['n_variables']
        dpv_score, dpv_cat = self._evaluate_inverse_scalar(dpv, (10, 20, 30))
        dpv_sug = "Collect more data or reduce number of poles." if dpv_cat == "Poor" else None
        metrics['dpv'] = MetricResult('dpv', dpv_score, dpv_cat, dpv, dpv_sug)
        if dpv_sug:
            suggestions.append(dpv_sug)

        # Scalar metrics
        for name, (g, e, gd, sug) in local_thresholds.items():
            val = report_vals.get(name, 0)
            sc, cat = self._evaluate_scalar(val, (g, e, gd))
            sugg = sug if cat == "Poor" else None
            metrics[name] = MetricResult(name, sc, cat, val, sugg)
            if sugg:
                suggestions.append(sugg)

        # Correlation
        corrs = report['correlations']
        if not corrs:
            corr_score, corr_cat, corr_sug = 100, "Low", None
            worst_pair, worst_val = None, 0.0
        else:
            worst_pair, worst_val = max(corrs.items(), key=lambda kv: abs(kv[1]))
            corr_score, corr_cat, corr_sug = self._evaluate_correlation(worst_val)

        corr_result = MetricResult(
            'correlation', corr_score, corr_cat,
            worst_val, corr_sug, details={'pair': worst_pair}
        )
        metrics['correlation'] = corr_result
        if corr_sug:
            suggestions.append(corr_sug)

        # Overall score
        total_weight = sum(self.weights.values())
        overall_score = sum(
            metrics[k].score * self.weights.get(k, 1)
            for k in metrics.keys()
            if k in self.weights
        ) / total_weight

        if overall_score >= 90:
            overall_cat = "Godly"
        elif overall_score >= 75:
            overall_cat = "Excellent"
        elif overall_score >= 60:
            overall_cat = "Good"
        else:
            overall_cat = "Poor"

        overall = MetricResult("overall", round(overall_score, 1), overall_cat, round(overall_score, 1))
        return ReportEvaluation(metrics=metrics, overall=overall, suggestions=suggestions, warnings=warnings)

# Global variables to store data
global_f_ghz = None
global_complex_epsilon = None
global_correlation_matrix = None
global_measured_dk = None
global_measured_df = None
global_n_poles = None
global_scale_real = None
global_scale_imag = None
global_sigma_dk = None
global_sigma_df = None
global_weights_real = None
global_weights_imag = None

def load_data(csv_content):
    global global_f_ghz, global_complex_epsilon, global_sigma_dk, global_sigma_df
    global global_weights_real, global_weights_imag
    try:
        data = pd.read_csv(io.StringIO(csv_content)).dropna()
        f_ghz = data.iloc[:, 0].values
        dk = data.iloc[:, 1].values
        df = data.iloc[:, 2].values

        # Check for uncertainty columns (columns 4 and 5)
        if data.shape[1] >= 5:
            # Uncertainties provided
            sigma_dk = data.iloc[:, 3].values
            sigma_df = data.iloc[:, 4].values
            global_sigma_dk = sigma_dk
            global_sigma_df = sigma_df

            # Calculate weights as 1/sigma^2 (variance weighting)
            # Avoid division by zero
            global_weights_real = np.where(sigma_dk > 0, 1.0 / sigma_dk**2, 1.0)
            global_weights_imag = np.where(sigma_df > 0, 1.0 / (sigma_df * dk)**2, 1.0)
        else:
            # No uncertainties provided - use default weighting
            global_sigma_dk = None
            global_sigma_df = None

            # No experimental σ → give both channels equal influence
            global_weights_real = np.ones_like(dk)
            global_weights_imag = np.ones_like(dk)   # <<< change

        # Normalize weights to have mean = 1 for each component
        global_weights_real = global_weights_real / np.mean(global_weights_real)
        global_weights_imag = global_weights_imag / np.mean(global_weights_imag)

        complex_epsilon = dk - 1j * (dk * df)
        global_f_ghz = f_ghz
        global_complex_epsilon = complex_epsilon
        return f_ghz, complex_epsilon
    except Exception as e:
        raise ValueError(f"Failed to parse CSV: {e}")

# The extra kwargs let us reuse the function elsewhere
def quick_single_fit(f_ghz, complex_epsilon, initial_params, n_poles,
                     max_iter=200, method='leastsq', seed=None):
    """Perform a quick fit for BIC calculation with limited iterations."""
    params = Parameters()

    # Optionally change optimiser and random seed
    if seed is not None:
        np.random.seed(seed)

    # Add epsilon infinity with data-driven bounds
    dk_data = np.real(complex_epsilon)
    dk_tail = np.median(dk_data[-max(3, int(0.1*len(dk_data))):])  # last 10% of points
    eps_inf_lower = max(0.10*dk_tail, 1.0)
    # Keep ε∞ below the static permittivity estimate to avoid un‑physical solutions
    eps_inf_upper = np.median(dk_data[:max(3, int(0.1*len(dk_data)))])  # ≈ εs
    params.add('eps_inf',
               value=initial_params['eps_inf'],
               min=eps_inf_lower,
               max=eps_inf_upper)

    # Add poles
    for i in range(n_poles):
        params.add(f'delta_eps_{i}',
                  value=initial_params.get(f'delta_eps_{i}', 0.5),
                  min=-dk_data.max(),   # permit small negative correction if
                  max= dk_data.max())   # ε∞ overshoots during optimisation

        # Use log-tau parameterization
        tau_i = initial_params.get(f'tau_{i}', 1e-9)
        log_tau_i = np.log10(tau_i)
        params.add(f'log_tau_{i}',
                  value=log_tau_i,
                  min=-15,
                  max=-6)

    # Add constraints for ordered relaxation times in log-space
    for i in range(1, n_poles):
        params.add(f'log_tau_ratio_{i}', value=0.3, min=np.log10(1.1), max=2.0)
        params[f'log_tau_{i}'].expr = f'log_tau_{i-1} + log_tau_ratio_{i}'

    # Quick minimization with limited iterations
    minimizer = Minimizer(residual, params, fcn_args=(f_ghz, complex_epsilon, n_poles))
    result = minimizer.minimize(method=method, max_nfev=max_iter)

    # Fallback to Nelder-Mead if first method fails
    if not result.success:
        result = minimizer.minimize(method='nelder', max_nfev=max_iter)

    # Calculate BIC using unscaled residuals
    # Get model prediction
    fitted_epsilon = calculate_model(result.params.valuesdict(), f_ghz)
    real_residuals = np.real(fitted_epsilon) - np.real(complex_epsilon)
    imag_residuals = np.imag(fitted_epsilon) - np.imag(complex_epsilon)

    # --- scale residuals exactly as in optimisation ---
    scale_real = np.max(np.abs(np.real(complex_epsilon))) or 1.0
    scale_imag = np.max(np.abs(np.imag(complex_epsilon))) or 1.0

    real_resid  = real_residuals / scale_real
    imag_resid  = imag_residuals / scale_imag

    unscaled_rss = np.sum(real_resid**2) + np.sum(imag_resid**2)

    n_data = len(real_residuals) + len(imag_residuals)
    n_params = n_poles * 2 + 1  # delta_eps and tau for each pole, plus eps_inf

    if n_data > 0 and unscaled_rss > 0:
        bic = n_params * np.log(n_data) + n_data * np.log(unscaled_rss / n_data)
    else:
        bic = 1e6  # Large but finite value for failed fits

    # Ensure BIC is finite for JSON serialization
    if np.isinf(bic) or np.isnan(bic):
        bic = 1e6

    return {'bic': float(bic), 'params': result.params.valuesdict(), 'rss': float(unscaled_rss)}

# Try multiple seeds and retain the best BIC for each n
def select_poles_bic(f_ghz, complex_epsilon, max_poles=4, n_starts=4):
    """Select optimal number of poles using Bayesian Information Criterion."""
    bic_best, best_n = 1e6, 1  # Use finite values to avoid JSON issues
    bic_values = {}

    for n in range(1, max_poles + 1):
        best_bic_n = 1e6  # Use finite values to avoid JSON issues
        for k in range(n_starts):
            try:
                ini = estimate_initial_parameters(
                    f_ghz, complex_epsilon, n, random_seed=np.random.randint(1e6))
                fit = quick_single_fit(f_ghz, complex_epsilon, ini, n,
                                       max_iter=200, method='nelder', seed=k)
                best_bic_n = min(best_bic_n, fit['bic'])
            except Exception:
                continue
        # Handle infinite BIC values for JSON serialization
        if np.isinf(best_bic_n) or np.isnan(best_bic_n):
            best_bic_n = 1e6  # Large but finite value for failed fits
        bic_values[n] = best_bic_n
        if best_bic_n < bic_best - 6:    # "strong" evidence threshold (Kass & Raftery, 1995)
            bic_best, best_n = best_bic_n, n

    logger.info(f"BIC values: {bic_values}")
    logger.info(f"Selected {best_n} poles with BIC = {bic_best:.2f}")

    return best_n, bic_values

def detect_poles_second_derivative(f_ghz, complex_epsilon):
    """Detect poles using second derivative of log(ε″)."""
    eps_imag = -np.imag(complex_epsilon)

    # Remove zero or negative values for log
    valid_mask = eps_imag > 1e-10
    if np.sum(valid_mask) < 10:
        return 1

    f_valid = f_ghz[valid_mask]
    eps_valid = eps_imag[valid_mask]

    # Take log and smooth
    log_eps = np.log(eps_valid)

    # Calculate derivatives using finite differences
    if len(log_eps) < 5:
        return 1

    # First derivative
    d1_log_eps = np.gradient(log_eps, f_valid)

    # Second derivative
    d2_log_eps = np.gradient(d1_log_eps, f_valid)

    # Find zero crossings of second derivative (inflection points)
    zero_crossings = []
    for i in range(1, len(d2_log_eps) - 1):
        if d2_log_eps[i-1] * d2_log_eps[i+1] < 0:  # Sign change
            zero_crossings.append(i)

    # Filter out closely spaced crossings
    if len(zero_crossings) > 1:
        filtered_crossings = [zero_crossings[0]]
        for zc in zero_crossings[1:]:
            if zc - filtered_crossings[-1] > len(d2_log_eps) // 10:  # Minimum separation
                filtered_crossings.append(zc)
        zero_crossings = filtered_crossings

    # Number of poles approximately equals number of inflection points
    n_poles = max(1, min(len(zero_crossings), 4))

    return n_poles

def detect_number_of_poles(f_ghz, complex_epsilon):
    """Auto-detect the number of Debye poles using multiple methods."""
    eps_imag = -np.imag(complex_epsilon)

    if len(eps_imag) < 5:
        return 1, "insufficient_data", {}

    detection_method = None

    # Method 1: BIC-based selection (most robust)
    try:
        n_poles_bic, bic_values = select_poles_bic(f_ghz, complex_epsilon, max_poles=4)
    except Exception as e:
        logger.warning(f"BIC method failed: {e}")
        n_poles_bic = None
        bic_values = {}

    # Method 2: Second derivative of log(ε″)
    try:
        n_poles_d2 = detect_poles_second_derivative(f_ghz, complex_epsilon)
    except Exception as e:
        logger.warning(f"Second derivative method failed: {e}")
        n_poles_d2 = None

    # Method 3: Peak finding (fallback, improved)
    try:
        # Smooth the data with adaptive window
        window = min(7, len(eps_imag) // 4)
        if window >= 3:
            smoothed = np.convolve(eps_imag, np.ones(window)/window, mode='valid')
        else:
            smoothed = eps_imag

                # Dynamic prominence and width
        noise_level = np.std(np.diff(smoothed))
        prominence = max(0.01 * np.max(smoothed), 2 * noise_level)
        width      = max(1, len(smoothed)//40)

        peaks, properties = find_peaks(smoothed, prominence=prominence,
                                       width=width, distance=width)

        # ----- NEW shoulder finder -----
        if len(peaks) < 2:
            d2 = np.gradient(np.gradient(smoothed))
            zc = np.where(np.sign(d2[:-1]) != np.sign(d2[1:]))[0] + 1
            peaks = np.unique(np.concatenate([peaks, zc]))
        n_poles_peaks = len(peaks)

        if n_poles_peaks == 0:
            n_poles_peaks = 1
        elif n_poles_peaks > 4:
            n_poles_peaks = 3
    except Exception as e:
        logger.warning(f"Peak finding method failed: {e}")
        n_poles_peaks = 1

    # Combine results with preference for BIC
    if n_poles_bic is not None:
        n_poles = n_poles_bic
        detection_method = "BIC"
        logger.info(f"Using BIC method: {n_poles} poles")
    elif n_poles_d2 is not None:
        n_poles = n_poles_d2
        detection_method = "second_derivative"
        logger.info(f"Using second derivative method: {n_poles} poles")
    else:
        n_poles = n_poles_peaks
        detection_method = "peak_finding"
        logger.info(f"Using peak finding method: {n_poles} poles")

    return n_poles, detection_method, bic_values

def estimate_tau_cole_cole(f_ghz, complex_epsilon, eps_inf=None, eps_s=None):
    """Estimate relaxation time using Cole-Cole transformation."""
    dk = np.real(complex_epsilon)

    # Estimate eps_inf and eps_s if not provided
    if eps_inf is None:
        # Use high-frequency limit (last 10% of data)
        high_freq_idx = int(0.9 * len(dk))
        eps_inf = np.mean(dk[high_freq_idx:]) if high_freq_idx < len(dk) else dk[-1]

    if eps_s is None:
        # Use low-frequency limit (first 10% of data)
        low_freq_idx = int(0.1 * len(dk))
        eps_s = np.mean(dk[:low_freq_idx]) if low_freq_idx > 0 else dk[0]

    # Avoid division by zero
    if abs(eps_s - eps_inf) < 1e-6:
        return None

    # Cole-Cole transformation
    # For Debye model: (ε - ε∞)/(εs - ε∞) = 1/(1 + jωτ)
    # Phase angle φ = arctan(ωτ), so φ = π/4 when ωτ = 1
    try:
        # Calculate normalized complex permittivity
        eps_norm = (complex_epsilon - eps_inf) / (eps_s - eps_inf)

        # Calculate phase angle
        phi = np.angle(eps_norm)

        # Find frequency where phase is closest to -π/4 (for Debye relaxation)
        target_phase = -np.pi / 4
        idx_min = np.argmin(np.abs(phi - target_phase))

        # Estimate tau from this frequency
        f_relax = f_ghz[idx_min]
        tau_est = 1 / (2 * np.pi * f_relax * 1e9)

        return tau_est
    except Exception as e:
        logger.warning(f"Cole-Cole tau estimation failed: {e}")
        return None

def find_loss_peaks_advanced(f_ghz, complex_epsilon, n_expected=None):
    """Find loss peaks with improved peak detection for overlapping relaxations."""
    eps_imag = -np.imag(complex_epsilon)

    if len(eps_imag) < 3:
        return []

    # Apply mild smoothing to reduce noise
    window = min(5, len(eps_imag) // 5)
    if window >= 3:
        eps_smooth = np.convolve(eps_imag, np.ones(window)/window, mode='same')
    else:
        eps_smooth = eps_imag

    # Find all local maxima
    peaks = []
    for i in range(1, len(eps_smooth) - 1):
        if eps_smooth[i] > eps_smooth[i-1] and eps_smooth[i] > eps_smooth[i+1]:
            peaks.append(i)

    if not peaks:
        # If no peaks found, use maximum
        peaks = [np.argmax(eps_smooth)]

    # Sort peaks by prominence
    peak_values = [eps_smooth[p] for p in peaks]
    sorted_indices = np.argsort(peak_values)[::-1]
    peaks = [peaks[i] for i in sorted_indices]

    # If we expect a certain number of peaks, try to find them
    if n_expected and len(peaks) < n_expected:
        # Look for shoulders or inflection points
        d2_eps = np.gradient(np.gradient(eps_smooth))
        for i in range(1, len(d2_eps) - 1):
            if d2_eps[i-1] > 0 and d2_eps[i+1] < 0:  # Inflection point
                if i not in peaks:
                    peaks.append(i)

    # Convert to frequencies
    peak_frequencies = [f_ghz[p] for p in peaks[:n_expected] if p < len(f_ghz)]

    return peak_frequencies

def estimate_initial_parameters(f_ghz, complex_epsilon, n_poles=None, random_seed=None):
    """Estimate initial parameters for Multi-Pole Debye model with improved tau estimation."""
    dk = np.real(complex_epsilon)
    df = -np.imag(complex_epsilon) / np.real(complex_epsilon)

    # Set random seed for reproducibility
    if random_seed is not None:
        np.random.seed(random_seed)

    # Auto-detect poles if not specified
    if n_poles is None or n_poles == 'auto':
        n_poles, _ = detect_number_of_poles(f_ghz, complex_epsilon)

    # Estimate eps_inf and eps_s more robustly
    # Use percentiles to be less sensitive to outliers
    high_freq_idx = int(0.8 * len(dk))
    low_freq_idx = int(0.2 * len(dk))

    eps_inf = np.median(dk[high_freq_idx:]) if high_freq_idx < len(dk) else dk[-1]
    eps_s = np.median(dk[:low_freq_idx]) if low_freq_idx > 0 else dk[0]

    # Ensure physical constraints
    eps_inf = max(1.0, eps_inf)
    eps_s = max(eps_inf + 0.1, eps_s)  # eps_s should be > eps_inf

    total_delta_eps = eps_s - eps_inf

    # Initialize parameters dictionary
    params = {'eps_inf': eps_inf, 'n_poles': n_poles}

    if n_poles == 1:
        # Single pole case - use Cole-Cole transformation
        tau_cc = estimate_tau_cole_cole(f_ghz, complex_epsilon, eps_inf, eps_s)

        if tau_cc is not None:
            params['tau_0'] = tau_cc
        else:
            # Fallback to maximum loss method
            max_loss_idx = np.argmax(-np.imag(complex_epsilon))
            f_max = f_ghz[max_loss_idx]
            params['tau_0'] = 1 / (2 * np.pi * f_max * 1e9)

        params['delta_eps_0'] = total_delta_eps

    else:
        # Multiple poles case
        # Try to find actual peak frequencies
        peak_freqs = find_loss_peaks_advanced(f_ghz, complex_epsilon, n_poles)

        if len(peak_freqs) >= n_poles:
            # Use detected peaks for tau values
            for i in range(n_poles):
                params[f'tau_{i}'] = 1 / (2 * np.pi * peak_freqs[i] * 1e9)
                params[f'delta_eps_{i}'] = total_delta_eps / n_poles
        else:
            # Fallback to logarithmic spacing, but biased toward detected peaks
            f_min = f_ghz[0] if len(f_ghz) > 0 else 0.1
            f_max = f_ghz[-1] if len(f_ghz) > 0 else 100.0

            if peak_freqs:
                # Use detected peaks as anchors
                f_min = min(f_min, min(peak_freqs) * 0.3)
                f_max = max(f_max, max(peak_freqs) * 3.0)

            # Create tau values with some randomization to avoid local minima
            log_f_min = np.log10(f_min)
            log_f_max = np.log10(f_max)

            # Add small random perturbations to help optimizer
            log_f_poles = np.linspace(log_f_min, log_f_max, n_poles)
            log_f_poles += np.random.uniform(-0.1, 0.1, n_poles) * (log_f_max - log_f_min) / n_poles
            f_poles = 10**log_f_poles

            # Assign parameters
            for i in range(n_poles):
                params[f'tau_{i}'] = 1 / (2 * np.pi * f_poles[i] * 1e9)

                                # Vary delta_eps slightly to help optimizer
                variation = 1.0 + np.random.uniform(-0.2, 0.2)
                params[f'delta_eps_{i}'] = (total_delta_eps / n_poles) * variation

        # Clamp individual delta_eps values to prevent negative values before normalization
        # Allow vanishingly small strengths; negative values are already forbidden
        min_delta_eps = 1e-6
        for i in range(n_poles):
            params[f'delta_eps_{i}'] = max(min_delta_eps, params[f'delta_eps_{i}'])

        # Ensure sum of delta_eps equals total
        sum_delta = sum(params[f'delta_eps_{i}'] for i in range(n_poles))
        if sum_delta > 0:
            for i in range(n_poles):
                params[f'delta_eps_{i}'] *= total_delta_eps / sum_delta

    return params

def calculate_model_from_params(params_dict):
    """Calculate Multi-Pole Debye model from parameters dictionary."""
    global global_f_ghz
    if global_f_ghz is None:
        return json.dumps({"error": "No data loaded"})

    f_ghz = global_f_ghz
    omega = 2 * np.pi * f_ghz * 1e9

    eps_inf = params_dict.get('eps_inf', 1.0)
    n_poles = params_dict.get('n_poles', 1)

    # Start with epsilon infinity
    complex_eps = np.full_like(f_ghz, eps_inf, dtype=complex)

    # Add each Debye pole
    for i in range(n_poles):
        delta_eps = params_dict.get(f'delta_eps_{i}', 0)
        tau = params_dict.get(f'tau_{i}', 1e-12)

        # Debye relaxation: Δε / (1 + jωτ)
        denominator = 1 + 1j * omega * tau
        complex_eps += delta_eps / denominator

    eps_prime = np.real(complex_eps)
    eps_double_prime = -np.imag(complex_eps)

    return json.dumps({
        "eps_prime": eps_prime.tolist(),
        "eps_double_prime": eps_double_prime.tolist()
    })

def calculate_model(params_dict, f_ghz):
    """Calculate Multi-Pole Debye model for given parameters and frequencies."""
    omega = 2 * np.pi * f_ghz * 1e9

    eps_inf = params_dict.get('eps_inf', 1.0)
    n_poles = params_dict.get('n_poles', 1)

    # Start with epsilon infinity
    complex_eps = np.full_like(f_ghz, eps_inf, dtype=complex)

    # Add each Debye pole
    for i in range(n_poles):
        delta_eps = params_dict.get(f'delta_eps_{i}', 0)

        # Handle both log_tau and tau parameters for compatibility
        if f'log_tau_{i}' in params_dict:
            tau = 10 ** params_dict[f'log_tau_{i}']
        else:
            tau = params_dict.get(f'tau_{i}', 1e-12)

        # Debye relaxation: Δε / (1 + jωτ)
        denominator = 1 + 1j * omega * tau
        complex_eps += delta_eps / denominator

    return complex_eps

def residual(params, f_ghz, complex_epsilon_data, n_poles):
    """Residual function for Multi-Pole Debye model with heteroscedastic weighting."""
    global global_scale_real, global_scale_imag, global_weights_real, global_weights_imag

    p_dict = params.valuesdict()
    p_dict['n_poles'] = n_poles
    model_epsilon = calculate_model(p_dict, f_ghz)

    # Calculate and store scaling factors on first call
    if global_scale_real is None:
        global_scale_real = np.max(np.abs(np.real(complex_epsilon_data))) or 1.0
        global_scale_imag = np.max(np.abs(np.imag(complex_epsilon_data))) or 1.0

    # Calculate unweighted residuals
    real_residual = (np.real(model_epsilon) - np.real(complex_epsilon_data))
    imag_residual = (np.imag(model_epsilon) - np.imag(complex_epsilon_data))

    # Apply scaling
    real_residual_scaled = real_residual / global_scale_real
    imag_residual_scaled = imag_residual / global_scale_imag

    # Apply weights if available
    if global_weights_real is not None and global_weights_imag is not None:
        real_residual_weighted = real_residual_scaled * np.sqrt(global_weights_real)
        imag_residual_weighted = imag_residual_scaled * np.sqrt(global_weights_imag)
    else:
        real_residual_weighted = real_residual_scaled
        imag_residual_weighted = imag_residual_scaled

    return np.concatenate([real_residual_weighted, imag_residual_weighted])

def calculate_jacobian(params_dict, f_ghz, n_poles, rel_step=1e-6, abs_step=1e-12):
    """Calculate Jacobian matrix using finite differences with proper scaling."""
    global global_scale_real, global_scale_imag, global_weights_real, global_weights_imag

    param_names = ['eps_inf']
    for i in range(n_poles):
        param_names.append(f'delta_eps_{i}')
        # Use log_tau if available, otherwise tau
        if f'log_tau_{i}' in params_dict:
            param_names.append(f'log_tau_{i}')
        else:
            param_names.append(f'tau_{i}')

    n_params = len(param_names)
    n_data = len(f_ghz) * 2  # real and imag parts

    jacobian = np.zeros((n_data, n_params))

    # Base model
    params_dict['n_poles'] = n_poles
    base_model = calculate_model(params_dict, f_ghz)

    # Use global scaling factors for consistency
    scale_real = global_scale_real if global_scale_real is not None else (np.max(np.abs(np.real(base_model))) or 1.0)
    scale_imag = global_scale_imag if global_scale_imag is not None else (np.max(np.abs(np.imag(base_model))) or 1.0)

    # Apply scaling and weights
    real_base = np.real(base_model) / scale_real
    imag_base = np.imag(base_model) / scale_imag

    if global_weights_real is not None and global_weights_imag is not None:
        real_base *= np.sqrt(global_weights_real)
        imag_base *= np.sqrt(global_weights_imag)

    base_residual = np.concatenate([real_base, imag_base])

    # Calculate derivatives
    for i, param_name in enumerate(param_names):
        if param_name in params_dict:
            param_value = params_dict[param_name]

            # Choose appropriate step size based on parameter type
            if param_name.startswith('log_tau'):
                # For log-tau parameters, use absolute step in log-space
                step = 1e-5  # Small step in log10 space (~0.002% change in tau)
            elif param_name.startswith('tau_'):
                # For regular tau parameters (backward compatibility)
                # Use larger absolute step to avoid machine precision issues
                step = max(rel_step * abs(param_value), 1e-10)
            else:
                # For other parameters (eps_inf, delta_eps)
                step = max(rel_step * abs(param_value), abs_step)

            params_plus = params_dict.copy()
            params_plus[param_name] += step

            model_plus = calculate_model(params_plus, f_ghz)
            real_plus = np.real(model_plus) / scale_real
            imag_plus = np.imag(model_plus) / scale_imag

            if global_weights_real is not None and global_weights_imag is not None:
                real_plus *= np.sqrt(global_weights_real)
                imag_plus *= np.sqrt(global_weights_imag)

            residual_plus = np.concatenate([real_plus, imag_plus])

            jacobian[:, i] = (residual_plus - base_residual) / step

    return jacobian

def calculate_unscaled_rss(real_residuals, imag_residuals):
    """Calculate unscaled RSS for comparable AIC/BIC across datasets."""
    global global_sigma_dk, global_sigma_df, global_measured_dk

    # Calculate unscaled sum of squares
    if global_sigma_dk is not None and global_sigma_df is not None:
        # If uncertainties are provided, normalize by variance
        # Use same variance model as weights: real by σ_dk², imag by (σ_df × Dk)²
        real_rss = np.sum((real_residuals / global_sigma_dk)**2)
        # For imaginary part, include Dk factor to match weighting scheme
        dk_values = np.array(global_measured_dk) if global_measured_dk is not None else np.ones_like(imag_residuals)
        imag_rss = np.sum((imag_residuals / (global_sigma_df * dk_values))**2)
    else:
        # Use unscaled residuals directly
        real_rss = np.sum(real_residuals**2)
        imag_rss = np.sum(imag_residuals**2)

    return real_rss + imag_rss

def calculate_correlation_matrix(params_dict, f_ghz, complex_epsilon_data, n_poles):
    """Calculate correlation matrix from Jacobian."""
    try:
        # Calculate Jacobian
        J = calculate_jacobian(params_dict, f_ghz, n_poles)

        # Calculate covariance matrix: (J^T J)^-1
        JTJ = np.dot(J.T, J)

        # Add small regularization to ensure positive definite
        JTJ += np.eye(JTJ.shape[0]) * 1e-10

        # Calculate covariance
        cov_matrix = np.linalg.inv(JTJ)

        # Convert to correlation matrix
        std_devs = np.sqrt(np.diag(cov_matrix))
        corr_matrix = np.zeros_like(cov_matrix)

        for i in range(len(std_devs)):
            for j in range(len(std_devs)):
                if std_devs[i] > 0 and std_devs[j] > 0:
                    corr_matrix[i, j] = cov_matrix[i, j] / (std_devs[i] * std_devs[j])
                else:
                    corr_matrix[i, j] = 0

        return corr_matrix
    except:
        return None

def format_correlation_matrix(corr_matrix, n_poles, use_log_tau=False):
    """Format correlation matrix for display."""
    if corr_matrix is None:
        return "Unable to calculate correlation matrix"

    param_names = ['eps_inf']
    for i in range(n_poles):
        param_names.append(f'delta_eps_{i}')
        if use_log_tau:
            param_names.append(f'log_tau_{i}')
        else:
            param_names.append(f'tau_{i}')

    result = "Correlations:\\n"

    try:
        for i, param1 in enumerate(param_names):
            for j, param2 in enumerate(param_names):
                if j > i:  # Only show upper triangle
                    if i < len(corr_matrix) and j < len(corr_matrix[0]):
                        correlation = corr_matrix[i, j]
                        if abs(correlation) > 0.1:  # Only show significant correlations
                            # Display log_tau as tau for clarity
                            display_param1 = param1.replace('log_tau_', 'tau_')
                            display_param2 = param2.replace('log_tau_', 'tau_')
                            result += f"    ({display_param1}, {display_param2}) = {correlation:+.4f}\\n"
    except (IndexError, TypeError) as e:
        result += f"    Error accessing correlation matrix: {e}\\n"

    return result

def create_updated_report(params_dict):
    """Create an updated report with current parameters."""
    global global_f_ghz, global_complex_epsilon, global_n_poles
    global global_scale_real, global_scale_imag, global_weights_real, global_weights_imag

    if global_f_ghz is None:
        return json.dumps({"error": "No data loaded"})

    n_poles = params_dict.get('n_poles', global_n_poles or 1)
    params_dict['n_poles'] = n_poles
    model_epsilon = calculate_model(params_dict, global_f_ghz)

    real_residuals = np.real(model_epsilon) - np.real(global_complex_epsilon)
    imag_residuals = np.imag(model_epsilon) - np.imag(global_complex_epsilon)

    # Use global scaling factors for consistency with optimization
    scale_real = global_scale_real if global_scale_real is not None else (np.max(np.abs(np.real(global_complex_epsilon))) or 1.0)
    scale_imag = global_scale_imag if global_scale_imag is not None else (np.max(np.abs(np.imag(global_complex_epsilon))) or 1.0)

    scaled_real_residuals = real_residuals / scale_real
    scaled_imag_residuals = imag_residuals / scale_imag

    # Apply weights if available
    if global_weights_real is not None and global_weights_imag is not None:
        weighted_real_residuals = scaled_real_residuals * np.sqrt(global_weights_real)
        weighted_imag_residuals = scaled_imag_residuals * np.sqrt(global_weights_imag)
    else:
        weighted_real_residuals = scaled_real_residuals
        weighted_imag_residuals = scaled_imag_residuals

    n_dat = len(real_residuals) + len(imag_residuals)
    n_var = 1 + 2 * n_poles  # eps_inf + (delta_eps, tau) for each pole

    # Use scaled residuals for chi-square (optimization metric)
    chi_sqr = np.sum(weighted_real_residuals**2) + np.sum(weighted_imag_residuals**2)
    red_chi_sqr = chi_sqr / (n_dat - n_var) if (n_dat - n_var) > 0 else 0

    # Use unscaled RSS for AIC/BIC (comparable across datasets)
    unscaled_rss = calculate_unscaled_rss(real_residuals, imag_residuals)
    aic = 2 * n_var + n_dat * np.log(unscaled_rss / n_dat) if n_dat > 0 else 0
    bic = n_var * np.log(n_dat) + n_dat * np.log(unscaled_rss / n_dat) if n_dat > 0 else 0

    # Calculate correlation matrix for current parameters
    corr_matrix = calculate_correlation_matrix(params_dict, global_f_ghz, global_complex_epsilon, n_poles)
    use_log_tau = any('log_tau_' in k for k in params_dict.keys())
    corr_text = format_correlation_matrix(corr_matrix, n_poles, use_log_tau)

    # Compute statistics for dynamic thresholds
    measured_df = -np.imag(global_complex_epsilon) / np.real(global_complex_epsilon)
    stats_block = {
        "median_dk": float(np.median(np.real(global_complex_epsilon))),
        "median_df": float(np.median(measured_df)),
        "peak_df": float(np.max(measured_df))
    }

    # Create JSON data structure
    json_data = {
        "model": "Multi-Pole Debye",
        "timestamp": datetime.now().isoformat(),
        "parameters": params_dict,
        "fit_statistics": {
            "n_data_points": n_dat,
            "n_variables": n_var,
            "chi_square": chi_sqr,
            "reduced_chi_square": red_chi_sqr,
            "unscaled_rss": unscaled_rss
        },
        "information_criteria": {
            "aic": aic,
            "bic": bic
        },
        "residual_analysis": {
            "real_part": {
                "mean": float(np.mean(real_residuals)),
                "std_dev": float(np.std(real_residuals)),
                "rms": float(np.sqrt(np.mean(real_residuals**2)))
            },
            "imaginary_part": {
                "mean": float(np.mean(imag_residuals)),
                "std_dev": float(np.std(imag_residuals)),
                "rms": float(np.sqrt(np.mean(imag_residuals**2)))
            }
        },
        "statistics": stats_block,
        "correlation_matrix": corr_matrix.tolist() if corr_matrix is not None else None
    }

    # Format poles information
    poles_info = ""
    for i in range(n_poles):
        delta_eps = params_dict.get(f'delta_eps_{i}', 0)
        tau = params_dict.get(f'tau_{i}', 0)
        f_relax = 1 / (2 * np.pi * tau) / 1e9 if tau > 0 else 0
        poles_info += f"    Pole {i+1}:\\n"
        poles_info += f"        Δε_{i} = {delta_eps:.4f}\\n"
        poles_info += f"        τ_{i}  = {tau:.4e} s\\n"
        poles_info += f"        f_relax = {f_relax:.4f} GHz\\n"

    report = f"""Multi-Pole Debye Fit Report
{'=' * 50}
Date: {datetime.now()}

Model and Parameters
--------------------------------------------------
ε*(ω) = ε_∞ + Σ[Δε_i / (1 + jωτ_i)]

Number of poles: {n_poles}

Using current values (manual):
    eps_inf = {params_dict.get('eps_inf', 0):.4f}

{poles_info}

Fit Statistics
    # data points = {n_dat}
    # variables   = {n_var}
    chi-square    = {chi_sqr:.4f}
    reduced chi-square = {red_chi_sqr:.4f}
    AIC              = {aic:.4f}
    BIC              = {bic:.4f}

Residual Analysis
    Real Part (Dk):
        Mean: {np.mean(real_residuals):.4f}
        Std Dev: {np.std(real_residuals):.4f}
        RMS: {np.sqrt(np.mean(real_residuals**2)):.4f}
    Imaginary Part (Loss Factor):
        Mean: {np.mean(imag_residuals):.4f}
        Std Dev: {np.std(imag_residuals):.4f}
        RMS: {np.sqrt(np.mean(imag_residuals**2)):.4f}

{corr_text}
"""
    return json.dumps({"report": report, "json_data": json_data})

def create_plotly_plot(f_ghz, measured_eps, fitted_eps):
    fig = make_subplots(rows=2, cols=1, subplot_titles=('Real Permittivity', 'Loss Tangent'))
    fig.add_trace(go.Scatter(x=f_ghz, y=np.real(measured_eps), mode='markers', name='Measured Dk', marker=dict(color='black')), row=1, col=1)
    fig.add_trace(go.Scatter(x=f_ghz, y=np.real(fitted_eps), mode='lines', name='Fitted Dk', line=dict(color='red')), row=1, col=1)
    measured_df = -np.imag(measured_eps) / np.real(measured_eps)
    fitted_df = -np.imag(fitted_eps) / np.real(fitted_eps)
    fig.add_trace(go.Scatter(x=f_ghz, y=measured_df, mode='markers', name='Measured Df', marker=dict(color='black'), showlegend=False), row=2, col=1)
    fig.add_trace(go.Scatter(x=f_ghz, y=fitted_df, mode='lines', name='Fitted Df', line=dict(color='red'), showlegend=False), row=2, col=1)
    fig.update_xaxes(title_text="Frequency (GHz)", row=1, col=1)
    fig.update_yaxes(title_text="Dielectric Constant (Dk)", row=1, col=1)
    fig.update_xaxes(title_text="Frequency (GHz)", row=2, col=1)
    fig.update_yaxes(title_text="Dissipation Factor (Df)", row=2, col=1)
    fig.update_layout(height=600, margin=dict(l=20, r=20, t=40, b=20), template="plotly_white", legend=dict(yanchor="top", y=0.99, xanchor="right", x=0.99))
    return pio.to_json(fig)

def evaluate_current_fit(params_dict):
    """Evaluate the current fit quality and return evaluation results."""
    global global_f_ghz, global_complex_epsilon, global_n_poles
    global global_scale_real, global_scale_imag, global_weights_real, global_weights_imag

    if global_f_ghz is None:
        return json.dumps({"error": "No data loaded"})

    n_poles = params_dict.get('n_poles', global_n_poles or 1)
    params_dict['n_poles'] = n_poles
    model_epsilon = calculate_model(params_dict, global_f_ghz)

    real_residuals = np.real(model_epsilon) - np.real(global_complex_epsilon)
    imag_residuals = np.imag(model_epsilon) - np.imag(global_complex_epsilon)

    # Use global scaling factors for consistency
    scale_real = global_scale_real if global_scale_real is not None else (np.max(np.abs(np.real(global_complex_epsilon))) or 1.0)
    scale_imag = global_scale_imag if global_scale_imag is not None else (np.max(np.abs(np.imag(global_complex_epsilon))) or 1.0)

    scaled_real_residuals = real_residuals / scale_real
    scaled_imag_residuals = imag_residuals / scale_imag

    # Apply weights if available
    if global_weights_real is not None and global_weights_imag is not None:
        weighted_real_residuals = scaled_real_residuals * np.sqrt(global_weights_real)
        weighted_imag_residuals = scaled_imag_residuals * np.sqrt(global_weights_imag)
    else:
        weighted_real_residuals = scaled_real_residuals
        weighted_imag_residuals = scaled_imag_residuals

    n_dat = len(real_residuals) + len(imag_residuals)
    n_var = 1 + 2 * n_poles

    # Use scaled residuals for chi-square
    chi_sqr = np.sum(weighted_real_residuals**2) + np.sum(weighted_imag_residuals**2)
    red_chi_sqr = chi_sqr / (n_dat - n_var) if (n_dat - n_var) > 0 else 0

    # Use unscaled RSS for information criteria
    unscaled_rss = calculate_unscaled_rss(real_residuals, imag_residuals)
    aic = 2 * n_var + n_dat * np.log(unscaled_rss / n_dat) if n_dat > 0 else 0
    bic = n_var * np.log(n_dat) + n_dat * np.log(unscaled_rss / n_dat) if n_dat > 0 else 0

    # Calculate correlation matrix
    corr_matrix = calculate_correlation_matrix(params_dict, global_f_ghz, global_complex_epsilon, n_poles)

    # Compute statistics for dynamic thresholds
    measured_df = -np.imag(global_complex_epsilon) / np.real(global_complex_epsilon)
    stats_block = {
        "median_dk": float(np.median(np.real(global_complex_epsilon))),
        "median_df": float(np.median(measured_df)),
        "peak_df": float(np.max(measured_df))
    }

    # Create JSON report format
    json_report = {
        "model": "Multi-Pole Debye",
        "timestamp": datetime.now().isoformat(),
        "parameters": params_dict,
        "fit_statistics": {
            "n_data_points": n_dat,
            "n_variables": n_var,
            "chi_square": chi_sqr,
            "reduced_chi_square": red_chi_sqr,
            "unscaled_rss": unscaled_rss
        },
        "information_criteria": {
            "aic": aic,
            "bic": bic
        },
        "residual_analysis": {
            "real_part": {
                "mean": float(np.mean(real_residuals)),
                "std_dev": float(np.std(real_residuals)),
                "rms": float(np.sqrt(np.mean(real_residuals**2)))
            },
            "imaginary_part": {
                "mean": float(np.mean(imag_residuals)),
                "std_dev": float(np.std(imag_residuals)),
                "rms": float(np.sqrt(np.mean(imag_residuals**2)))
            }
        },
        "statistics": stats_block,
        "correlation_matrix": corr_matrix.tolist() if corr_matrix is not None else None
    }

    # Evaluate the fit
    try:
        evaluator = FitEvaluator()
        report_data = FitEvaluator.from_json_report(json_report)
        evaluation = evaluator.evaluate(report_data)
        json_report['evaluation'] = evaluation.to_dict()
    except Exception as e:
        logger.warning(f"Could not evaluate fit: {e}")
        json_report['evaluation'] = None

    return json.dumps(json_report)

def create_downloadable_plot(params_dict):
    global global_f_ghz, global_complex_epsilon, global_n_poles
    if global_f_ghz is None:
        return None

    n_poles = params_dict.get('n_poles', global_n_poles or 1)
    params_dict['n_poles'] = n_poles
    fitted_epsilon = calculate_model(params_dict, global_f_ghz)

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))

    ax1.plot(global_f_ghz, np.real(global_complex_epsilon), 'ko', label='Measured')
    ax1.plot(global_f_ghz, np.real(fitted_epsilon), 'r-', lw=2, label='Fitted')
    ax1.set_xlabel('Frequency (GHz)')
    ax1.set_ylabel('Dielectric Constant (Dk)')
    ax1.set_title('Real Permittivity')
    ax1.legend()
    ax1.grid(True, alpha=0.5)

    measured_df = -np.imag(global_complex_epsilon) / np.real(global_complex_epsilon)
    fitted_df = -np.imag(fitted_epsilon) / np.real(fitted_epsilon)
    ax2.plot(global_f_ghz, measured_df, 'ko', label='Measured')
    ax2.plot(global_f_ghz, fitted_df, 'r-', lw=2, label='Fitted')
    ax2.set_xlabel('Frequency (GHz)')
    ax2.set_ylabel('Dissipation Factor (Df)')
    ax2.set_title('Loss Tangent')
    ax2.legend()
    ax2.grid(True, alpha=0.5)

    plt.tight_layout()

    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=150)
    plt.close(fig)
    buf.seek(0)
    return buf.getvalue()

def run_analysis(csv_content, n_poles_input, method):
    global global_f_ghz, global_complex_epsilon, global_correlation_matrix
    global global_measured_dk, global_measured_df, global_n_poles
    global global_scale_real, global_scale_imag

    # Reset scaling factors for new data
    global_scale_real = None
    global_scale_imag = None

    f_ghz, complex_epsilon_data = load_data(csv_content)

    # Store measured data globally
    global_measured_dk = np.real(complex_epsilon_data).tolist()
    global_measured_df = (-np.imag(complex_epsilon_data) / np.real(complex_epsilon_data)).tolist()

    # Determine number of poles
    detection_method_used = None
    bic_values = {}
    if n_poles_input == 'auto':
        n_poles, detection_method_used, bic_values = detect_number_of_poles(f_ghz, complex_epsilon_data)
        print(f"Auto-detected {n_poles} pole(s) using {detection_method_used} method")
    else:
        n_poles = int(n_poles_input)
        detection_method_used = "manual"

    global_n_poles = n_poles

    initial_params = estimate_initial_parameters(f_ghz, complex_epsilon_data, n_poles, random_seed=42)
    params = Parameters()

    # Add epsilon infinity with data-driven bounds
    dk_data = np.real(complex_epsilon_data)
    dk_tail = np.median(dk_data[-max(3, int(0.1*len(dk_data))):])  # last 10% of points
    eps_inf_lower = max(0.10*dk_tail, 1.0)
    # Keep ε∞ below the static permittivity estimate to avoid un‑physical solutions
    eps_inf_upper = np.median(dk_data[:max(3, int(0.1*len(dk_data)))])  # ≈ εs
    params.add('eps_inf',
               value=initial_params['eps_inf'],
               min=eps_inf_lower,
               max=eps_inf_upper)

    # Add poles
    for i in range(n_poles):
        # Relaxation strength - allow negative correction for ε∞ overshoot
        params.add(f'delta_eps_{i}',
                  value=initial_params[f'delta_eps_{i}'],
                  min=-dk_data.max(),   # permit small negative correction if
                  max= dk_data.max())   # ε∞ overshoots during optimisation

        # Use log-tau parameterization for better optimization
        tau_i = initial_params[f'tau_{i}']
        log_tau_i = np.log10(tau_i)
        params.add(f'log_tau_{i}',
                  value=log_tau_i,
                  min=-15,  # 10^-15 seconds
                  max=-6)   # 10^-6 seconds

    # Add constraints to order relaxation times using log-space
    # log_tau_i = log_tau_{i-1} + log_ratio_i where log_ratio >= log10(1.1)
    for i in range(1, n_poles):
        params.add(f'log_tau_ratio_{i}', value=0.3, min=np.log10(1.1), max=2.0)
        params[f'log_tau_{i}'].expr = f'log_tau_{i-1} + log_tau_ratio_{i}'

    minimizer = Minimizer(residual, params, fcn_args=(f_ghz, complex_epsilon_data, n_poles))
    result = minimizer.minimize(method=method)

    # --- NEW --------------------------------------------------
    if result.covar is None:
        try:
            # result.params already holds the best values
            def f(p):
                # Create temporary params object with updated values
                temp_params = result.params.copy()
                for i, name in enumerate(result.var_names):
                    temp_params[name].value = p[i]
                return residual(temp_params, f_ghz, complex_epsilon_data, n_poles)

            J = nd.Jacobian(f)(result.x)
            cov = np.linalg.inv(J.T @ J)
            result.covar = cov
        except Exception as err:
            logger.warning(f'Could not compute covariance via numdifftools: {err}')
    # ----------------------------------------------------------

    # Extract fitted parameters, converting log_tau back to tau
    fit_params = {}
    for p, v in result.params.items():
        if p.startswith('log_tau_ratio'):
            continue  # Skip log_tau_ratio parameters
        elif p.startswith('log_tau_'):
            # Convert log_tau back to tau
            pole_idx = p.split('_')[-1]
            fit_params[f'tau_{pole_idx}'] = float(10 ** v.value)
        else:
            fit_params[p] = float(v.value)
    fit_params['n_poles'] = n_poles

    # Calculate AIC and BIC using consistent scaling and weights
    fitted_epsilon = calculate_model(result.params.valuesdict(), f_ghz)
    real_residuals = np.real(fitted_epsilon) - np.real(complex_epsilon_data)
    imag_residuals = np.imag(fitted_epsilon) - np.imag(complex_epsilon_data)

    # Use global scaling factors for consistency
    scale_real = global_scale_real if global_scale_real is not None else (np.max(np.abs(np.real(complex_epsilon_data))) or 1.0)
    scale_imag = global_scale_imag if global_scale_imag is not None else (np.max(np.abs(np.imag(complex_epsilon_data))) or 1.0)

    scaled_real_residuals = real_residuals / scale_real
    scaled_imag_residuals = imag_residuals / scale_imag

    # Apply weights if available
    if global_weights_real is not None and global_weights_imag is not None:
        weighted_real_residuals = scaled_real_residuals * np.sqrt(global_weights_real)
        weighted_imag_residuals = scaled_imag_residuals * np.sqrt(global_weights_imag)
    else:
        weighted_real_residuals = scaled_real_residuals
        weighted_imag_residuals = scaled_imag_residuals

    n_dat = len(real_residuals) + len(imag_residuals)
    n_var = 1 + 2 * n_poles

    # Use unscaled RSS for AIC/BIC to ensure comparability
    unscaled_rss = calculate_unscaled_rss(real_residuals, imag_residuals)
    aic = 2 * n_var + n_dat * np.log(unscaled_rss / n_dat) if n_dat > 0 else 0
    bic = n_var * np.log(n_dat) + n_dat * np.log(unscaled_rss / n_dat) if n_dat > 0 else 0

    # Extract correlation matrix from fit result
    correlations_str = ""
    if hasattr(result, 'var_names') and hasattr(result, 'covar') and result.covar is not None:
        # Filter out ratio parameters and get indices of kept parameters
        kept_params = []
        kept_indices = []
        for i, name in enumerate(result.var_names):
            if not ('log_tau_ratio' in name):
                kept_params.append(name)
                kept_indices.append(i)

        # Extract the submatrix for kept parameters only
        n_kept = len(kept_indices)
        filtered_covar = np.zeros((n_kept, n_kept))
        for i, idx_i in enumerate(kept_indices):
            for j, idx_j in enumerate(kept_indices):
                filtered_covar[i, j] = result.covar[idx_i, idx_j]

        global_correlation_matrix = filtered_covar

        # Calculate correlations from filtered covariance matrix
        correlations_str = "\\nCorrelations (from optimization):\\n"
        for i in range(n_kept):
            for j in range(i + 1, n_kept):
                try:
                    std_i = np.sqrt(filtered_covar[i, i])
                    std_j = np.sqrt(filtered_covar[j, j])
                    if std_i > 0 and std_j > 0:
                        corr_val = filtered_covar[i, j] / (std_i * std_j)
                        if abs(corr_val) > 0.1:
                            # Display log_tau as tau for clarity
                            display_name1 = kept_params[i].replace('log_tau_', 'tau_')
                            display_name2 = kept_params[j].replace('log_tau_', 'tau_')
                            correlations_str += f"    ({display_name1}, {display_name2}) = {corr_val:+.4f}\\n"
                except Exception as e:
                    logger.warning(f"Error calculating correlation: {e}")

    # Generate standard LMFit table
    report_content = fit_report(result).replace('[[', '').replace(']]', '')

    # Format poles information
    poles_info = ""
    for i in range(n_poles):
        delta_eps = fit_params[f'delta_eps_{i}']
        tau = fit_params[f'tau_{i}']
        f_relax = 1 / (2 * np.pi * tau) / 1e9 if tau > 0 else 0
        poles_info += f"    Pole {i+1}:\\n"
        poles_info += f"        Δε_{i} = {delta_eps:.4f}\\n"
        poles_info += f"        τ_{i}  = {tau:.4e} s\\n"
        poles_info += f"        f_relax = {f_relax:.4f} GHz\\n"

    model_section = f"""Model and Parameters
--------------------------------------------------
ε*(ω) = ε_∞ + Σ[Δε_i / (1 + jωτ_i)]

Number of poles: {n_poles}

Using fitted values:
    eps_inf = {fit_params['eps_inf']:.4f}

{poles_info}

Information Criteria:
    AIC = {aic:.4f}
    BIC = {bic:.4f}
"""

    full_report = f"Multi-Pole Debye Fit Report\\n{'=' * 50}\\nDate: {datetime.now()}\\n\\n{model_section}\\n\\n{report_content}"
    plot_json = create_plotly_plot(f_ghz, complex_epsilon_data, fitted_epsilon)

    return_data = {
        "report": full_report,
        "plot_json": plot_json,
        "f_ghz": f_ghz.tolist(),
        "fitted_params": fit_params,
        "correlations": correlations_str,
        "measured_dk": global_measured_dk,
        "measured_df": global_measured_df,
        "n_poles": n_poles,
        "detection_method": detection_method_used,
        "bic_values": bic_values
    }
    return json.dumps(return_data)
`;

        async function main() {
            setLoadingState(true, 'Initializing Environment...');
            try {
                pyodide = await loadPyodide({
                    indexURL: "https://cdn.jsdelivr.net/pyodide/v0.27.0/full/"
                });
                setLoadingState(true, 'Loading Python Packages...');
                await pyodide.loadPackage(['numpy', 'pandas', 'micropip', 'matplotlib', 'scipy']);
                const micropip = pyodide.pyimport('micropip');
                setLoadingState(true, 'Installing Python wheels...');
                await micropip.install(['lmfit==1.3.1',          // pin for reproducibility
                                       'numdifftools==0.9.41',   // parameter uncertainties
                                       'plotly']);
                isPyodideReady = true;
                await pyodide.runPythonAsync(pythonScript);
                setLoadingState(false);
            } catch (err) {
                isPyodideReady = false;
                showError(`Failed to initialize Python environment: ${err}`);
                setLoadingState(false);
            }
        }

        function setLoadingState(isLoading, message = '') {
            statusMessage.textContent = message;
            statusDiv.style.display = isLoading ? 'flex' : 'none';
            runButton.disabled = isLoading;
            runButton.classList.toggle('btn-disabled', isLoading);
            if (!isLoading) updateButtonState();
        }

        function showError(message) {
            errorMessage.textContent = message;
            errorBox.classList.remove('hidden');
            outputDiv.classList.add('hidden');
        }

        function hideError() {
            errorBox.classList.add('hidden');
        }

        function updateButtonState() {
            const isReady = isPyodideReady && !!fileContent;
            runButton.disabled = !isReady;
            runButton.classList.toggle('btn-disabled', !isReady);
        }

        function createPoleControls(nPoles, params) {
            poleControlsContainer.innerHTML = '';
            currentPoleControls = {};

            for (let i = 0; i < nPoles; i++) {
                const poleDiv = document.createElement('div');
                poleDiv.className = 'pole-controls';
                poleDiv.innerHTML = `
                    <h4 class="text-sm font-semibold text-gray-700 mb-3">Pole ${i + 1}</h4>
                    <div class="space-y-4">
                        <div>
                            <label for="delta_eps_${i}_slider" class="block text-xs font-medium text-gray-600" title="Δε ≥ 0 (physical constraint)">Δε<sub>${i}</sub></label>
                            <input type="range" id="delta_eps_${i}_slider" class="mt-1">
                            <input type="number" id="delta_eps_${i}_input" class="mt-1 w-full p-1 border border-gray-300 rounded-md text-sm">
                        </div>
                        <div>
                            <label for="tau_${i}_slider" class="block text-xs font-medium text-gray-600">τ<sub>${i}</sub> (s)</label>
                            <input type="range" id="tau_${i}_slider" class="mt-1" step="any">
                            <input type="number" id="tau_${i}_input" class="mt-1 w-full p-1 border border-gray-300 rounded-md text-sm" step="any">
                        </div>
                    </div>
                `;
                poleControlsContainer.appendChild(poleDiv);

                // Store references and set up controls
                const deltaEpsSlider = document.getElementById(`delta_eps_${i}_slider`);
                const deltaEpsInput = document.getElementById(`delta_eps_${i}_input`);
                const tauSlider = document.getElementById(`tau_${i}_slider`);
                const tauInput = document.getElementById(`tau_${i}_input`);

                // Set up delta_eps controls (enforce physical constraint Δε ≥ 0)
                const deltaEps = params[`delta_eps_${i}`] || 0.5;
                const deltaEpsMin = Math.min(0.1, deltaEps * 0.5); // Small positive minimum for flexibility
                const deltaEpsMax = Math.max(5, deltaEps * 2);
                deltaEpsSlider.min = deltaEpsMin;
                deltaEpsSlider.max = deltaEpsMax;
                deltaEpsSlider.step = 0.01;
                deltaEpsSlider.value = deltaEps;
                deltaEpsInput.min = deltaEpsMin;
                deltaEpsInput.max = deltaEpsMax;
                deltaEpsInput.step = 0.01;
                deltaEpsInput.value = deltaEps;
                deltaEpsInput.title = "Δε ≥ 0 (physical constraint)";

                // Set up tau controls (log scale for slider)
                const tau = params[`tau_${i}`] || 1e-9;
                const logTauMin = -15;  // 10^-15 seconds
                const logTauMax = -6;   // 10^-6 seconds
                const logTau = Math.log10(tau);

                tauSlider.min = logTauMin;
                tauSlider.max = logTauMax;
                tauSlider.step = 0.1;
                tauSlider.value = logTau;

                tauInput.min = 1e-15;
                tauInput.max = 1e-6;
                tauInput.step = 1e-15;
                tauInput.value = tau;

                // Store controls
                currentPoleControls[`delta_eps_${i}`] = {
                    slider: deltaEpsSlider,
                    input: deltaEpsInput
                };
                currentPoleControls[`tau_${i}`] = {
                    slider: tauSlider,
                    input: tauInput,
                    isLog: true
                };

                // Add event listeners
                deltaEpsSlider.addEventListener('input', (e) => {
                    deltaEpsInput.value = e.target.value;
                    updateUIFromControls();
                });

                deltaEpsInput.addEventListener('input', (e) => {
                    deltaEpsSlider.value = e.target.value;
                    updateUIFromControls();
                });

                tauSlider.addEventListener('input', (e) => {
                    const tau = Math.pow(10, parseFloat(e.target.value));
                    tauInput.value = tau.toExponential(2);
                    updateUIFromControls();
                });

                tauInput.addEventListener('input', (e) => {
                    const tau = parseFloat(e.target.value);
                    if (tau > 0) {
                        tauSlider.value = Math.log10(tau);
                        updateUIFromControls();
                    }
                });
            }
        }

        function setupParameterControls(params) {
            const nPoles = params.n_poles || 1;
            currentNPoles = nPoles;

            // Set up epsilon infinity control
            const epsInfSlider = document.getElementById('eps_inf_slider');
            const epsInfInput = document.getElementById('eps_inf_input');

            const epsInf = params.eps_inf || 1.0;
            epsInfSlider.min = 1;
            epsInfSlider.max = Math.max(epsInf * 2, 10);
            epsInfSlider.step = 0.01;
            epsInfSlider.value = epsInf;

            epsInfInput.min = 1;
            epsInfInput.max = Math.max(epsInf * 2, 10);
            epsInfInput.step = 0.01;
            epsInfInput.value = epsInf;

            // Create pole controls
            createPoleControls(nPoles, params);

            // Add event listeners for epsilon infinity
            epsInfSlider.addEventListener('input', (e) => {
                epsInfInput.value = e.target.value;
                updateUIFromControls();
            });

            epsInfInput.addEventListener('input', (e) => {
                epsInfSlider.value = e.target.value;
                updateUIFromControls();
            });
        }

        async function updateUIFromControls() {
            if (!currentData || !pyodide) return;

            const params = {
                eps_inf: parseFloat(document.getElementById('eps_inf_input').value),
                n_poles: currentNPoles
            };

            // Collect pole parameters
            for (let i = 0; i < currentNPoles; i++) {
                params[`delta_eps_${i}`] = parseFloat(document.getElementById(`delta_eps_${i}_input`).value);
                params[`tau_${i}`] = parseFloat(document.getElementById(`tau_${i}_input`).value);
            }

            try {
                // Update Plot
                const modelJson = await pyodide.runPythonAsync(`
                    import json
                    params_dict = ${JSON.stringify(params)}
                    result = calculate_model_from_params(params_dict)
                    result
                `);
                const modelResults = JSON.parse(modelJson);

                if (modelResults.error) return;

                const fittedDk = modelResults.eps_prime;
                const fittedDf = fittedDk.map((dk, i) => modelResults.eps_double_prime[i] / dk);
                Plotly.restyle('plot-output', { y: [fittedDk, fittedDf] }, [1, 3]);

                // Update Report
                const reportJson = await pyodide.runPythonAsync(`
                    params_dict = ${JSON.stringify(params)}
                    create_updated_report(params_dict)
                `);
                const reportData = JSON.parse(reportJson);
                reportOutput.textContent = reportData.report;

                // Update fit quality indicator
                const evaluationJson = await pyodide.runPythonAsync(`
                    params_dict = ${JSON.stringify(params)}
                    evaluate_current_fit(params_dict)
                `);
                const evaluationData = JSON.parse(evaluationJson);

                if (evaluationData.evaluation) {
                    updateFitQualityIndicator(evaluationData.evaluation);
                }

            } catch (err) {
                console.error('Error updating UI:', err);
            }
        }

        function updateFitQualityIndicator(evaluation) {
            const indicator = document.getElementById('fit-quality-indicator');
            const scoreSpan = document.getElementById('fit-quality-score');
            const bar = document.getElementById('fit-quality-bar');
            const warningsDiv = document.getElementById('fit-warnings');
            const warningsContainer = document.getElementById('warnings-container');

            if (!evaluation) {
                indicator.classList.add('hidden');
                return;
            }

            indicator.classList.remove('hidden');
            const score = evaluation.overall_score;
            const category = evaluation.overall_category;

            scoreSpan.textContent = `${score} (${category})`;

            // Update bar width and color
            bar.style.width = `${score}%`;

            if (score >= 90) {
                bar.className = 'h-2 rounded-full transition-all duration-300 bg-green-500';
                indicator.className = 'mb-4 p-3 rounded-lg bg-green-50 border border-green-200';
            } else if (score >= 75) {
                bar.className = 'h-2 rounded-full transition-all duration-300 bg-blue-500';
                indicator.className = 'mb-4 p-3 rounded-lg bg-blue-50 border border-blue-200';
            } else if (score >= 60) {
                bar.className = 'h-2 rounded-full transition-all duration-300 bg-yellow-500';
                indicator.className = 'mb-4 p-3 rounded-lg bg-yellow-50 border border-yellow-200';
            } else {
                bar.className = 'h-2 rounded-full transition-all duration-300 bg-red-500';
                indicator.className = 'mb-4 p-3 rounded-lg bg-red-50 border border-red-200';
            }

            // Display warnings
            if (evaluation.warnings && evaluation.warnings.length > 0) {
                warningsDiv.classList.remove('hidden');
                warningsContainer.innerHTML = '';

                evaluation.warnings.forEach(warning => {
                    const badge = document.createElement('div');
                    badge.className = 'inline-block bg-orange-100 text-orange-800 text-xs px-2 py-1 rounded-full border border-orange-200 mr-1 mb-1';
                    badge.textContent = warning.length > 50 ? warning.substring(0, 50) + '...' : warning;
                    badge.title = warning; // Full text on hover
                    warningsContainer.appendChild(badge);
                });
            } else {
                warningsDiv.classList.add('hidden');
            }
        }

        function resetParameters() {
            if (!currentData || !currentData.fitted_params) return;
            setupParameterControls(currentData.fitted_params);

            const plotData = JSON.parse(currentData.plot_json);
            Plotly.restyle('plot-output', {
                y: [plotData.data[1].y, plotData.data[3].y]
            }, [1, 3]);
            reportOutput.textContent = currentData.report;

            // Update fit quality for reset parameters
            pyodide.runPythonAsync(`
                params_dict = ${JSON.stringify(currentData.fitted_params)}
                evaluate_current_fit(params_dict)
            `).then(evaluationJson => {
                const evaluationData = JSON.parse(evaluationJson);
                if (evaluationData.evaluation) {
                    updateFitQualityIndicator(evaluationData.evaluation);
                }
            });
        }

        async function downloadResults() {
            if (!currentData) return;
            setLoadingState(true, 'Preparing download...');
            try {
                const zip = new JSZip();
                const baseFilename = originalFilename.replace(/\.csv$/i, '');

                // Collect current parameters
                const params = {
                    eps_inf: parseFloat(document.getElementById('eps_inf_input').value),
                    n_poles: currentNPoles
                };

                for (let i = 0; i < currentNPoles; i++) {
                    params[`delta_eps_${i}`] = parseFloat(document.getElementById(`delta_eps_${i}_input`).value);
                    params[`tau_${i}`] = parseFloat(document.getElementById(`tau_${i}_input`).value);
                }

                // 1. Create Fitted Data CSV
                const modelJson = await pyodide.runPythonAsync(`
                    import json
                    params_dict = ${JSON.stringify(params)}
                    result = calculate_model_from_params(params_dict)
                    result
                `);
                const modelResults = JSON.parse(modelJson);

                let csvContent = "Frequency_GHz,fitted_Dk,fitted_Df\n";
                currentData.f_ghz.forEach((f, i) => {
                    const dk = modelResults.eps_prime[i];
                    const df = dk === 0 ? 0 : -modelResults.eps_double_prime[i] / dk;
                    csvContent += `${f},${dk},${df}\n`;
                });
                zip.file(`${baseFilename}_fitted.csv`, csvContent);

                // 2. Add Matplotlib Plot Image
                const imageBytes = await pyodide.runPythonAsync(`
                    params_dict = ${JSON.stringify(params)}
                    plot_bytes = create_downloadable_plot(params_dict)
                    list(plot_bytes) if plot_bytes else []
                `);
                if (imageBytes && imageBytes.length > 0) {
                    const imageData = new Uint8Array(imageBytes);
                    zip.file(`${baseFilename}_plot.png`, imageData);
                }

                // 3. Add Report
                zip.file(`${baseFilename}_report.txt`, reportOutput.textContent);

                // 4. Generate JSON report
                const reportJson = await pyodide.runPythonAsync(`
                    params_dict = ${JSON.stringify(params)}
                    create_updated_report(params_dict)
                `);
                const reportData = JSON.parse(reportJson);

                // Get evaluation
                const evaluationJson = await pyodide.runPythonAsync(`
                    params_dict = ${JSON.stringify(params)}
                    evaluate_current_fit(params_dict)
                `);
                const evaluationData = JSON.parse(evaluationJson);

                // Add additional fields to JSON
                const fullJsonReport = {
                    ...reportData.json_data,
                    input_file: originalFilename,
                    data: {
                        frequency_ghz: currentData.f_ghz,
                        measured: {
                            dk: currentData.measured_dk || [],
                            df: currentData.measured_df || []
                        },
                        fitted: {
                            dk: modelResults.eps_prime,
                            df: modelResults.eps_prime.map((dk, i) => {
                                return dk === 0 ? 0 : -modelResults.eps_double_prime[i] / dk;
                            })
                        }
                    },
                    evaluation: evaluationData.evaluation
                };

                zip.file(`${baseFilename}_report.json`, JSON.stringify(fullJsonReport, null, 2));

                // 5. Add evaluation report
                if (evaluationData.evaluation && evaluationData.evaluation.markdown_table) {
                    zip.file(`${baseFilename}_evaluation.md`, evaluationData.evaluation.markdown_table);
                }

                // Generate and Download Zip
                const content = await zip.generateAsync({ type: "blob" });
                const link = document.createElement("a");
                link.href = URL.createObjectURL(content);
                link.download = `${baseFilename}_multipole_debye_results.zip`;
                document.body.appendChild(link);
                link.click();
                document.body.removeChild(link);

            } catch(err) {
                showError(`Failed to create download package: ${err}`);
            } finally {
                setLoadingState(false);
            }
        }

        // Set up event listeners
        fileUpload.addEventListener('change', (event) => {
            const file = event.target.files[0];
            if (file) {
                originalFilename = file.name;
                const reader = new FileReader();
                reader.onload = (e) => {
                    fileContent = e.target.result;
                    updateButtonState();
                };
                reader.readAsText(file);
            } else {
                fileContent = null;
                originalFilename = '';
                updateButtonState();
            }
        });

        runButton.addEventListener('click', async () => {
            if (!fileContent || !pyodide) return;
            hideError();
            setLoadingState(true, 'Running analysis...');
            outputDiv.classList.add('hidden');

            try {
                const fitMethod = fitMethodSelect.value;
                const nPoles = nPolesSelect.value;

                // Pass the CSV content through a global variable to avoid escaping issues
                await pyodide.runPythonAsync(`
import json
global_csv_content = ${JSON.stringify(fileContent)}
result_json = run_analysis(global_csv_content, '${nPoles}', '${fitMethod}')
result_json
                `).then(async (resultJson) => {
                    const result = JSON.parse(resultJson);

                    currentData = result;

                    const plotData = JSON.parse(result.plot_json);
                    plotOutput.innerHTML = '';
                    Plotly.newPlot('plot-output', plotData.data, plotData.layout, {responsive: true});

                    currentData.measured_dk = result.measured_dk;
                    currentData.measured_df = result.measured_df;

                    reportOutput.textContent = result.report;
                    setupParameterControls(result.fitted_params);

                    // Show BIC chart if available
                    if (result.bic_values && Object.keys(result.bic_values).length > 1) {
                        const labels = Object.keys(result.bic_values).map(k => k + ' poles');
                        const bicVals = Object.values(result.bic_values);
                        const minBic = Math.min(...bicVals);
                        const deltaBic = bicVals.map(b => b - minBic); // ΔBIC relative to best

                        Plotly.newPlot('bic-chart',
                            [{
                                x: labels,
                                y: deltaBic,
                                type: 'bar',
                                marker: {
                                    color: deltaBic.map(d => d === 0 ? '#3b82f6' : '#94a3b8'),
                                    opacity: 0.7
                                },
                                text: deltaBic.map(d => d.toFixed(1)),
                                textposition: 'auto'
                            }],
                            {
                                title: 'ΔBIC vs. Pole Count',
                                yaxis: {title: 'ΔBIC (relative to best)'},
                                xaxis: {title: 'Model Complexity'},
                                margin: {l: 50, r: 20, t: 40, b: 40},
                                font: {size: 12}
                            },
                            {responsive: true}
                        );
                        document.getElementById('bic-chart-container').classList.remove('hidden');
                    } else {
                        document.getElementById('bic-chart-container').classList.add('hidden');
                    }

                                                            // Show detection method if auto-detected
                    if (result.detection_method && result.detection_method !== 'manual') {
                        const methodNames = {
                            'BIC': 'Bayesian Information Criterion',
                            'second_derivative': 'Second Derivative of log(ε″)',
                            'peak_finding': 'Peak Finding (improved)',
                            'insufficient_data': 'Insufficient Data'
                        };
                        const methodName = methodNames[result.detection_method] || result.detection_method;

                        let infoHTML = `<strong>Auto-detection:</strong> ${result.n_poles} pole(s) detected using ${methodName}`;

                        // Add BIC values if available
                        if (result.bic_values && Object.keys(result.bic_values).length > 0) {
                            infoHTML += '<br><strong>BIC values:</strong> ';
                            const bicEntries = [];
                            for (const [poles, bic] of Object.entries(result.bic_values)) {
                                bicEntries.push(`${poles}p: ${parseFloat(bic).toFixed(1)}`);
                            }
                            infoHTML += bicEntries.join(', ');
                        }

                        // Check if auto-detection div already exists
                        let infoDiv = document.getElementById('auto-detection-info');
                        if (infoDiv) {
                            // Update existing div
                            infoDiv.innerHTML = infoHTML;
                        } else {
                            // Create new div
                            infoDiv = document.createElement('div');
                            infoDiv.id = 'auto-detection-info';
                            infoDiv.className = 'mt-4 p-3 bg-blue-50 border border-blue-200 rounded-lg text-sm';
                            infoDiv.innerHTML = infoHTML;
                            document.getElementById('controls-panel').insertBefore(infoDiv, document.getElementById('controls-panel').firstChild);
                        }
                    } else {
                        // Remove auto-detection div if manual mode is used
                        const existingInfoDiv = document.getElementById('auto-detection-info');
                        if (existingInfoDiv) {
                            existingInfoDiv.remove();
                        }
                    }

                    // Show initial fit quality
                    const evaluationJson = await pyodide.runPythonAsync(`
                        params_dict = ${JSON.stringify(result.fitted_params)}
                        evaluate_current_fit(params_dict)
                    `);
                    const evaluationData = JSON.parse(evaluationJson);

                    if (evaluationData.evaluation) {
                        updateFitQualityIndicator(evaluationData.evaluation);
                    }

                    outputDiv.classList.remove('hidden');
                });

            } catch (err) {
                console.error(err);
                showError(`An error occurred during analysis: ${err.message}`);
            } finally {
                setLoadingState(false);
            }
        });

        resetButton.addEventListener('click', resetParameters);
        downloadButton.addEventListener('click', downloadResults);

        // Initialize
        main();
        updateButtonState();
    </script>
</body>
</html>